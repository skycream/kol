#!/usr/bin/env python3
"""
data í´ë” ë‚´ì˜ ëª¨ë“  json íŒŒì¼ì„ ì°¾ì•„ì„œ DBì— ì‚½ì…í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python process_all_json.py                    # ê¸°ë³¸ê°’ ì‚¬ìš©
    python process_all_json.py --data-dir custom # ì»¤ìŠ¤í…€ data ë””ë ‰í† ë¦¬
    python process_all_json.py --db-path test.db # ì»¤ìŠ¤í…€ DB ê²½ë¡œ
"""

import os
import sys
import json
import sqlite3
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime

# json_inserter ëª¨ë“ˆ ì„í¬íŠ¸
from json_inserter import JSONDatabaseInserter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(f'json_processing_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)


class JSONProcessor:
    """data í´ë” ë‚´ì˜ JSON íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ DBì— ì‚½ì…í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: str = "data", db_path: str = "db.db"):
        """
        Args:
            data_dir: data í´ë” ê²½ë¡œ
            db_path: SQLite DB íŒŒì¼ ê²½ë¡œ
        """
        self.data_dir = Path(data_dir)
        self.db_path = db_path
        self.processed_files = []
        self.duplicate_files = []
        self.error_files = []
        
    def find_json_files(self) -> List[Path]:
        """
        data í´ë” ë‚´ì˜ ëª¨ë“  json íŒŒì¼ì„ ì°¾ìŒ
        
        Returns:
            JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        json_files = []
        
        if not self.data_dir.exists():
            logger.error(f"data ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.data_dir}")
            return json_files
            
        # data/*/json/*.json íŒ¨í„´ìœ¼ë¡œ íŒŒì¼ ì°¾ê¸°
        for id_dir in self.data_dir.iterdir():
            if id_dir.is_dir():
                json_dir = id_dir / "json"
                if json_dir.exists():
                    for json_file in json_dir.glob("*.json"):
                        json_files.append(json_file)
                        
        return sorted(json_files)
        
    def process_json_file(self, json_path: Path, inserter: JSONDatabaseInserter) -> Tuple[bool, str]:
        """
        ê°œë³„ JSON íŒŒì¼ì„ ì²˜ë¦¬
        
        Args:
            json_path: JSON íŒŒì¼ ê²½ë¡œ
            inserter: JSONDatabaseInserter ì¸ìŠ¤í„´ìŠ¤
            
        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€) íŠœí”Œ
        """
        try:
            # JSON íŒŒì¼ ì½ê¸°
            with open(json_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
                
            # ê³µê³ ID ì¶”ì¶œ
            ê³µê³ ID = json_data.get('ê¸°ë³¸ì •ë³´', {}).get('ê³µê³ ID')
            if not ê³µê³ ID:
                return False, "ê³µê³ IDê°€ ì—†ìŠµë‹ˆë‹¤"
                
            # ì¤‘ë³µ ì²´í¬
            if inserter.check_duplicate_announcement(str(ê³µê³ ID)):
                return False, f"ê³µê³ ID {ê³µê³ ID}ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤"
                
            # DBì— ì‚½ì…
            result = inserter.insert_json_data(json_data)
            
            if result:
                return True, f"ê³µê³ ID {ê³µê³ ID} ì‚½ì… ì„±ê³µ"
            else:
                return False, "ì‚½ì… ì‹¤íŒ¨"
                
        except json.JSONDecodeError as e:
            return False, f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
        except Exception as e:
            return False, f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
            
    def process_all(self) -> Dict[str, int]:
        """
        ëª¨ë“  JSON íŒŒì¼ì„ ì²˜ë¦¬
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        # JSON íŒŒì¼ ì°¾ê¸°
        json_files = self.find_json_files()
        
        if not json_files:
            logger.warning("ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {"total": 0, "success": 0, "duplicate": 0, "error": 0}
            
        logger.info(f"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # DB ì—°ê²°
        try:
            conn = sqlite3.connect(self.db_path)
            conn.execute("PRAGMA foreign_keys = ON")
            inserter = JSONDatabaseInserter(conn)
            
            # ê° íŒŒì¼ ì²˜ë¦¬
            for json_file in json_files:
                logger.info(f"\nì²˜ë¦¬ ì¤‘: {json_file}")
                
                success, message = self.process_json_file(json_file, inserter)
                
                if success:
                    self.processed_files.append((json_file, message))
                    logger.info(f"âœ… {message}")
                elif "ì´ë¯¸ ì¡´ì¬" in message:
                    self.duplicate_files.append((json_file, message))
                    logger.warning(f"âš ï¸ {message}")
                else:
                    self.error_files.append((json_file, message))
                    logger.error(f"âŒ {message}")
                    
        except sqlite3.Error as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        finally:
            if 'conn' in locals():
                conn.close()
                
        # ê²°ê³¼ í†µê³„ ë°˜í™˜
        return {
            "total": len(json_files),
            "success": len(self.processed_files),
            "duplicate": len(self.duplicate_files),
            "error": len(self.error_files)
        }
        
    def print_summary(self, stats: Dict[str, int]):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        print(f"ğŸ“ ì´ íŒŒì¼ ìˆ˜: {stats['total']}")
        print(f"âœ… ì„±ê³µ: {stats['success']}")
        print(f"âš ï¸  ì¤‘ë³µ: {stats['duplicate']}")
        print(f"âŒ ì˜¤ë¥˜: {stats['error']}")
        print("="*60)
        
        # ìƒì„¸ ë‚´ì—­
        if self.processed_files:
            print("\nâœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ íŒŒì¼:")
            for file_path, message in self.processed_files:
                print(f"  - {file_path.name}: {message}")
                
        if self.duplicate_files:
            print("\nâš ï¸  ì¤‘ë³µìœ¼ë¡œ ê±´ë„ˆë›´ íŒŒì¼:")
            for file_path, message in self.duplicate_files:
                print(f"  - {file_path.name}: {message}")
                
        if self.error_files:
            print("\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí•œ íŒŒì¼:")
            for file_path, message in self.error_files:
                print(f"  - {file_path.name}: {message}")


def check_db_exists(db_path: str) -> bool:
    """DB íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    if not os.path.exists(db_path):
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
        logger.info("ë¨¼ì € pg_schema.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        logger.info("ì‹¤í–‰ ëª…ë ¹: python pg_schema.py")
        return False
    return True


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="data í´ë” ë‚´ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì°¾ì•„ì„œ DBì— ì‚½ì…í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python process_all_json.py                        # ê¸°ë³¸ê°’ ì‚¬ìš©
  python process_all_json.py --data-dir custom_data # ì»¤ìŠ¤í…€ data ë””ë ‰í† ë¦¬
  python process_all_json.py --db-path custom.db    # ì»¤ìŠ¤í…€ DB íŒŒì¼
  python process_all_json.py --dry-run              # ì‹¤ì œ ì‚½ì…í•˜ì§€ ì•Šê³  í™•ì¸ë§Œ
        """
    )
    
    parser.add_argument(
        "--data-dir",
        default="data",
        help="data ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: data)"
    )
    
    parser.add_argument(
        "--db-path",
        default="db.db",
        help="SQLite DB íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: db.db)"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ì‚½ì…í•˜ì§€ ì•Šê³  ì²˜ë¦¬ ëŒ€ìƒë§Œ í™•ì¸"
    )
    
    args = parser.parse_args()
    
    # DB íŒŒì¼ í™•ì¸
    if not check_db_exists(args.db_path):
        return
        
    # Dry run ëª¨ë“œ
    if args.dry_run:
        print("ğŸ” DRY RUN ëª¨ë“œ - ì‹¤ì œ ì‚½ì…í•˜ì§€ ì•Šê³  ëŒ€ìƒë§Œ í™•ì¸í•©ë‹ˆë‹¤.")
        processor = JSONProcessor(args.data_dir, args.db_path)
        json_files = processor.find_json_files()
        
        print(f"\nğŸ“ data ë””ë ‰í† ë¦¬: {args.data_dir}")
        print(f"ğŸ’¾ DB íŒŒì¼: {args.db_path}")
        print(f"ğŸ“‹ ë°œê²¬ëœ JSON íŒŒì¼: {len(json_files)}ê°œ")
        
        if json_files:
            print("\nJSON íŒŒì¼ ëª©ë¡:")
            for idx, json_file in enumerate(json_files, 1):
                # ID ì¶”ì¶œ (ìƒìœ„ ë””ë ‰í† ë¦¬ëª…)
                id_dir = json_file.parent.parent.name
                print(f"{idx:3d}. ID: {id_dir:10s} - {json_file.name}")
        return
        
    # ì‹¤ì œ ì²˜ë¦¬
    print(f"ğŸš€ JSON íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"ğŸ“ data ë””ë ‰í† ë¦¬: {args.data_dir}")
    print(f"ğŸ’¾ DB íŒŒì¼: {args.db_path}")
    print("="*60)
    
    processor = JSONProcessor(args.data_dir, args.db_path)
    stats = processor.process_all()
    processor.print_summary(stats)
    
    print("\nâœ¨ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()